{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1g-dto4glQIUjkGXH3TdPu5AeWd6dMgoo","timestamp":1739127095616}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Establecer acceso a Google Drive"],"metadata":{"id":"Uv8wAJ7B763N"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kq-IL7MgDeV_","executionInfo":{"status":"ok","timestamp":1741370988696,"user_tz":-60,"elapsed":20616,"user":{"displayName":"Marcos Cifuentes","userId":"03726672031011744657"}},"outputId":"b24d0ba9-3084-4363-df02-50b784dab662"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# 📌 Paso 1: Instalar las librerias necesarias"],"metadata":{"id":"1YpeqfAr72NC"}},{"cell_type":"code","source":["# 📌 Paso 1: Instalar las librerias necesarias\n","!pip install --upgrade fsspec==2024.9.0\n","!pip install scikit-learn nltk spacy pandas matplotlib datasets==3.1.0\n","!python -m spacy download es_core_news_sm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WPmH4BtYePNz","executionInfo":{"status":"ok","timestamp":1741371012337,"user_tz":-60,"elapsed":20201,"user":{"displayName":"Marcos Cifuentes","userId":"03726672031011744657"}},"outputId":"643ffd11-54ac-4c2a-98e2-3b7d4113199a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting fsspec==2024.9.0\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: fsspec\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n","torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n","torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n","torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n","torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n","torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n","torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n","torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n","torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n","torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n","torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed fsspec-2024.9.0\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Collecting datasets==3.1.0\n","  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (3.17.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets==3.1.0)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (4.67.1)\n","Collecting xxhash (from datasets==3.1.0)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets==3.1.0)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.1.0) (2024.9.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (3.11.13)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (0.28.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==3.1.0) (6.0.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==3.1.0) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets==3.1.0) (4.12.2)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.1.0) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.1.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.1.0) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==3.1.0) (2025.1.31)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n","Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n","Successfully installed datasets-3.1.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n","Collecting es-core-news-sm==3.7.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.15.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.67.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.10.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.27.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2025.1.31)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n","Installing collected packages: es-core-news-sm\n","Successfully installed es-core-news-sm-3.7.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('es_core_news_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}]},{"cell_type":"markdown","source":["# 📌 Paso 2: Cargar un conjunto de datos de sentimiento real en español"],"metadata":{"id":"d643LbsB8E7t"}},{"cell_type":"code","source":["import pandas as pd\n","import requests\n","from io import StringIO\n","from datasets import Dataset\n","\n","ID_DEL_ARCHIVO = '1gyN4UdZ2PTjtgeu6ZE-EBMJ1Y_0b5cI-'  # Enlace del csv guardado en drive\n","\n","url = f'https://drive.google.com/uc?id={ID_DEL_ARCHIVO}'\n","\n","\n","response = requests.get(url)\n","response.raise_for_status()  # Lanza una excepción para errores HTTP (4xx o 5xx)\n","csv_content = response.content.decode('utf-8')  # Decodifica el contenido a UTF-8\n","dataset = pd.read_csv(StringIO(csv_content))\n","\n","df = pd.DataFrame(dataset) # Transformo a dataframe el dataset\n","print(\"Primeras filas del DataFrame de Pandas:\")\n","print(df.head())\n","\n","print(\"\\nColumnas del DataFrame:\")\n","print(df.columns)\n","\n","print(df.shape) # Tamaño del dataframe\n","\n","print(len(df)) # Filas del dataframe"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-aQ6zVUaazKc","executionInfo":{"status":"ok","timestamp":1741371026684,"user_tz":-60,"elapsed":10925,"user":{"displayName":"Marcos Cifuentes","userId":"03726672031011744657"}},"outputId":"09fbc072-4a1e-466f-de6d-d6dc0fbb4db3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Primeras filas del DataFrame de Pandas:\n","   Unnamed: 0                                             tweets   labels\n","0           0  ChatGPT: Optimizing Language Models for Dialog...  neutral\n","1           1  Try talking with ChatGPT, our new AI system wh...     good\n","2           2  ChatGPT: Optimizing Language Models for Dialog...  neutral\n","3           3  THRILLED to share that ChatGPT, our new model ...     good\n","4           4  As of 2 minutes ago, @OpenAI released their ne...      bad\n","\n","Columnas del DataFrame:\n","Index(['Unnamed: 0', 'tweets', 'labels'], dtype='object')\n","(219294, 3)\n","219294\n"]}]},{"cell_type":"code","source":["df = df.drop(columns=[\"Unnamed: 0\"]) # Elimino la columna"],"metadata":{"id":"_INtzAsWk9pL","executionInfo":{"status":"ok","timestamp":1741371031702,"user_tz":-60,"elapsed":8,"user":{"displayName":"Marcos Cifuentes","userId":"03726672031011744657"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["print(df.head()) # Comprobacion\n","print(df.columns)\n","print(df.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZuVy-NVHlOU4","executionInfo":{"status":"ok","timestamp":1741371035046,"user_tz":-60,"elapsed":18,"user":{"displayName":"Marcos Cifuentes","userId":"03726672031011744657"}},"outputId":"9bd9f391-e5d8-408a-9b84-7d8128981e3e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["                                              tweets   labels\n","0  ChatGPT: Optimizing Language Models for Dialog...  neutral\n","1  Try talking with ChatGPT, our new AI system wh...     good\n","2  ChatGPT: Optimizing Language Models for Dialog...  neutral\n","3  THRILLED to share that ChatGPT, our new model ...     good\n","4  As of 2 minutes ago, @OpenAI released their ne...      bad\n","Index(['tweets', 'labels'], dtype='object')\n","(219294, 2)\n"]}]},{"cell_type":"markdown","source":["# 📌 Paso 3: Limpieza y preprocesamiento del texto"],"metadata":{"id":"xO26o6D48JV_"}},{"cell_type":"code","source":["import os\n","print(os.cpu_count())  # Número total de núcleos de Google Colab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5cQ0jybhdD-P","executionInfo":{"status":"ok","timestamp":1741371039019,"user_tz":-60,"elapsed":15,"user":{"displayName":"Marcos Cifuentes","userId":"03726672031011744657"}},"outputId":"168a1dfa-11c2-491e-9337-3d298a3db5fe"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n"]}]},{"cell_type":"code","source":["# 📌 Paso 3: Limpieza y preprocesamiento del texto\n","import spacy\n","\n","nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n","# 1) Cambiamos el \"es\" por \"en\" (texto en ingles)\n","# 2) Deshabilita análisis sintáctico y NER (Named Entity Recognition) para mejorar rendimiento\n","\n","def preprocess_texts(texts):\n","    processed_texts = []\n","    for doc in nlp.pipe((text.lower() for text in texts), batch_size=1000, n_process=2): # nlp.pipe() es mucho más eficiente que procesar cada fila individualmente con apply(), ya que procesa los textos en lotes, aprovechando el paralelismo.\n","      yield \" \".join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha])  #  No almacena todos los textos en memoria intermedia\n","\n","df['clean_text'] = list(preprocess_texts(df['tweets']))  # Convierte generador en lista\n","print(df.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4kqI-wdEFiWU","executionInfo":{"status":"ok","timestamp":1741371595125,"user_tz":-60,"elapsed":553909,"user":{"displayName":"Marcos Cifuentes","userId":"03726672031011744657"}},"outputId":"5f98b25a-5faa-4e12-b3d2-a9d7ef0e1323"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["                                              tweets   labels  \\\n","0  ChatGPT: Optimizing Language Models for Dialog...  neutral   \n","1  Try talking with ChatGPT, our new AI system wh...     good   \n","2  ChatGPT: Optimizing Language Models for Dialog...  neutral   \n","3  THRILLED to share that ChatGPT, our new model ...     good   \n","4  As of 2 minutes ago, @OpenAI released their ne...      bad   \n","\n","                                          clean_text  \n","0           chatgpt optimize language model dialogue  \n","1  try talk chatgpt new ai system optimize dialog...  \n","2  chatgpt optimize language model dialogue ai ma...  \n","3  thrilled share chatgpt new model optimize dial...  \n","4           minute ago release new chatgpt use right  \n"]}]},{"cell_type":"code","source":["# El proceso anterior tarda 16' por lo que vamos a guardar el dataframe resultante en un csv\n","# Guardar el DataFrame en la carpeta \"SA\" dentro de \"Colab Notebooks\"\n","\n","df.to_csv(\"/content/drive/My Drive/Colab_Notebooks/SA/clean_file.csv\", index=False)\n","\n","print(\"DataFrame guardado en Google Drive en la carpeta SA\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-5cqzzdofBLH","executionInfo":{"status":"ok","timestamp":1741373705770,"user_tz":-60,"elapsed":1629,"user":{"displayName":"Marcos Cifuentes","userId":"03726672031011744657"}},"outputId":"14e679d9-bd8d-432c-bd1a-a7e979702631"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["DataFrame guardado en Google Drive en la carpeta SA\n"]}]},{"cell_type":"code","source":["# Importar el nuevo csv\n","import pandas as pd\n","import requests\n","from io import StringIO\n","from datasets import Dataset\n","\n","ID_DEL_ARCHIVO = '18lLl4rnbUfpbXpUVKp2FsYv9Lm_OQibA'  # Enlace del csv guardado en drive\n","\n","url = f'https://drive.google.com/uc?id={ID_DEL_ARCHIVO}'\n","\n","\n","response = requests.get(url)\n","response.raise_for_status()  # Lanza una excepción para errores HTTP (4xx o 5xx)\n","csv_content = response.content.decode('utf-8')  # Decodifica el contenido a UTF-8\n","dataset = pd.read_csv(StringIO(csv_content))\n","\n","df = pd.DataFrame(dataset) # Transformo a dataframe el dataset\n","print(\"Primeras filas del DataFrame de Pandas:\")\n","print(df.head())\n","\n","print(\"\\nColumnas del DataFrame:\")\n","print(df.columns)\n","\n","print(df.shape) # Tamaño del dataframe"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hkIwozHV216e","executionInfo":{"status":"ok","timestamp":1741373793801,"user_tz":-60,"elapsed":22153,"user":{"displayName":"Marcos Cifuentes","userId":"03726672031011744657"}},"outputId":"48f6fc3f-2f57-45c8-bb2a-291546963d66"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Primeras filas del DataFrame de Pandas:\n","                                              tweets   labels  \\\n","0  ChatGPT: Optimizing Language Models for Dialog...  neutral   \n","1  Try talking with ChatGPT, our new AI system wh...     good   \n","2  ChatGPT: Optimizing Language Models for Dialog...  neutral   \n","3  THRILLED to share that ChatGPT, our new model ...     good   \n","4  As of 2 minutes ago, @OpenAI released their ne...      bad   \n","\n","                                          clean_text  \n","0           chatgpt optimize language model dialogue  \n","1  try talk chatgpt new ai system optimize dialog...  \n","2  chatgpt optimize language model dialogue ai ma...  \n","3  thrilled share chatgpt new model optimize dial...  \n","4           minute ago release new chatgpt use right  \n","\n","Columnas del DataFrame:\n","Index(['tweets', 'labels', 'clean_text'], dtype='object')\n","(219294, 3)\n"]}]},{"cell_type":"code","source":["print(df['clean_text'].isna().sum())  # Cuenta cuántos NaN hay\n","df = df.dropna(subset=['clean_text']) # Esto eliminará solo las filas donde clean_text es NaN, sin afectar otras columnas."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uM-S_4uStwhE","executionInfo":{"status":"ok","timestamp":1741373803224,"user_tz":-60,"elapsed":76,"user":{"displayName":"Marcos Cifuentes","userId":"03726672031011744657"}},"outputId":"61af33ad-00be-4789-99b2-cd2face19ad2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["286\n"]}]},{"cell_type":"markdown","source":["# 📌 Paso 4: Extracción de características (convertir texto en características TF-IDF)"],"metadata":{"id":"vx5Act9x8MGG"}},{"cell_type":"code","source":["# 📌 Paso 4: Extracción de características (convertir texto en características TF((Término Frecuencia)-IDF(Frecuencia inversa de documentos))\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer( # Crea el vectorizador TF-IDF con un máximo de 5000 palabras\n","    max_features=5000,  # Usa las 5000 palabras más relevantes\n","    min_df=5,  # Ignora palabras que aparecen en menos de 5 documentos\n","    max_df=0.9,  # Ignora palabras que aparecen en más del 90% de los documentos\n","    stop_words='english'  # Elimina palabras muy comunes en inglés\n",")\n","vectorizer = TfidfVectorizer(max_features=5000)\n","X = vectorizer.fit_transform(df['clean_text']) # Transforma los textos en una matriz numérica (TF-IDF)\n","y = df['labels'] # Extraccion de las etiquetas\n","print(X.shape) #  Muestra el tamaño de la matriz TF-IDF\n","print(X[:, :100].toarray()) # Imprime las primeras 100 columnas\n","print(y)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vQvZYFPzj1GV","executionInfo":{"status":"ok","timestamp":1741373815607,"user_tz":-60,"elapsed":2779,"user":{"displayName":"Marcos Cifuentes","userId":"03726672031011744657"}},"outputId":"8e1c9a7f-b504-45d9-c57c-6002ad39ab91"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["(219008, 5000)\n","[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n","0         neutral\n","1            good\n","2         neutral\n","3            good\n","4             bad\n","           ...   \n","219289        bad\n","219290       good\n","219291        bad\n","219292        bad\n","219293    neutral\n","Name: labels, Length: 219008, dtype: object\n"]}]},{"cell_type":"markdown","source":["# 📌 Paso 5: Selección de características por puntuación avg_tfidf"],"metadata":{"id":"fczLKCC-dZKo"}},{"cell_type":"code","source":["# Selección de características por puntuación avg_tfidf\n","\n","import numpy as np\n","\n","NTopFeatures = 100\n","\n","feature_names = vectorizer.get_feature_names_out() # Devuelve una lista con las palabras clave que corresponden a las columnas de X.\n","\n","# Calcular la puntuación media TF-IDF de cada palabra en todos los tweets\n","avg_tfidf_scores = X.mean(axis=0).A1  #  A1 convierte la matriz en un array 1D\n","\n","# Ordena las características por puntuación media TF-IDF en orden descendente\n","top_features_indices = np.argsort(avg_tfidf_scores)[::-1][:NTopFeatures]\n","\n","# Obtener el nombre de las palabras principales\n","top_features = [feature_names[i] for i in top_features_indices]\n","\n","# Muestra las NTopFeatures palabras\n","print(f\"Top {NTopFeatures} most significant features:\")\n","for feature in top_features:\n","    print(feature)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fMYeUQIYHRq8","executionInfo":{"status":"ok","timestamp":1741373817842,"user_tz":-60,"elapsed":48,"user":{"displayName":"Marcos Cifuentes","userId":"03726672031011744657"}},"outputId":"214c793d-0341-4636-f825-079a94e305fb"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Top 100 most significant features:\n","chatgpt\n","ai\n","write\n","ask\n","openai\n","google\n","like\n","new\n","good\n","use\n","chatbot\n","know\n","code\n","think\n","answer\n","try\n","question\n","go\n","thing\n","get\n","time\n","work\n","search\n","need\n","generate\n","people\n","human\n","way\n","future\n","create\n","well\n","tool\n","come\n","world\n","day\n","help\n","bot\n","talk\n","chat\n","learn\n","change\n","job\n","prompt\n","replace\n","play\n","year\n","story\n","model\n","amp\n","look\n","tell\n","language\n","want\n","technology\n","give\n","amazing\n","tech\n","find\n","great\n","fun\n","content\n","twitter\n","start\n","gpt\n","build\n","take\n","artificialintelligence\n","essay\n","game\n","intelligence\n","say\n","right\n","love\n","interesting\n","read\n","post\n","explain\n","article\n","feel\n","mind\n","make\n","let\n","response\n","artificial\n","thank\n","blow\n","tweet\n","text\n","conversation\n","poem\n","today\n","business\n","test\n","pretty\n","idea\n","lot\n","real\n","see\n","result\n","open\n"]}]},{"cell_type":"code","source":["# Filtremos el conjunto de datos X por las 100 características principales\n","# Obtener los índices de las características más importantes del vocabulario\n","top_features_indices = [np.where(vectorizer.get_feature_names_out() == feature)[0][0] for feature in top_features]\n","\n","# Filtrar la matriz X para conservar sólo las columnas correspondientes a las características más destacadas.\n","X_filtered = X[:, top_features_indices]\n","\n","# Imprime la forma de la matriz filtrada\n","print(\"Shape of filtered X:\", X_filtered.shape)\n","print(X_filtered[:, :100].toarray())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d1dVg_THHqag","executionInfo":{"status":"ok","timestamp":1741373840085,"user_tz":-60,"elapsed":171,"user":{"displayName":"Marcos Cifuentes","userId":"03726672031011744657"}},"outputId":"bf9bfd57-c873-4394-970c-f9632474df85"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of filtered X: (219008, 100)\n","[[0.09366825 0.         0.         ... 0.         0.         0.        ]\n"," [0.06418643 0.15351886 0.         ... 0.         0.         0.        ]\n"," [0.03909153 0.18699549 0.         ... 0.         0.         0.        ]\n"," ...\n"," [1.         0.         0.         ... 0.         0.         0.        ]\n"," [0.06987795 0.         0.         ... 0.         0.         0.        ]\n"," [0.10932583 0.         0.         ... 0.         0.         0.        ]]\n"]}]},{"cell_type":"markdown","source":["# 📌 Paso 6: Estadística del conjunto de datos de nuevo (sin incluir la característica seleccionada)"],"metadata":{"id":"asnc3MZxfarq"}},{"cell_type":"code","source":["# Mezcla aleatoriamente las filas del DataFrame\n","df = df.sample(frac = 1, random_state=42)\n","\n","# Mostramos informacion del dataset\n","print(df.head(), df['labels'].value_counts())\n","print(df['labels'].unique())\n","print(df.describe())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iewmHqoSj1gq","executionInfo":{"status":"ok","timestamp":1741373844302,"user_tz":-60,"elapsed":726,"user":{"displayName":"Marcos Cifuentes","userId":"03726672031011744657"}},"outputId":"74d6263e-7615-4acd-b62d-c74d036bc4aa"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                   tweets   labels  \\\n","72302   everyone running #revit api requests through #...  neutral   \n","143132  ChatGPT and other generative ML systems repres...      bad   \n","138712  I never thought creativity can be automated, u...  neutral   \n","66074   Grammarly keknya bakal kelindes ChatGPT juga :...  neutral   \n","129704  Using ChatGPT at least 2-3 times a day now for...     good   \n","\n","                                               clean_text  \n","72302                  run revit api request chatgpt like  \n","143132  chatgpt generative ml system represent step ch...  \n","138712      think creativity automate see amp try chatgpt  \n","66074   grammarly keknya bakal kelindes chatgpt juga f...  \n","129704  chatgpt time day genuinely useful stuff esp us...   labels\n","bad        107563\n","good        55985\n","neutral     55460\n","Name: count, dtype: int64\n","['neutral' 'bad' 'good']\n","         tweets  labels clean_text\n","count    219008  219008     219008\n","unique   217338       3     188703\n","top     ChatGPT     bad    chatgpt\n","freq        122  107563       2280\n"]}]},{"cell_type":"markdown","source":["# 📌 Paso 7: Entrenar y evaluar modelos ML"],"metadata":{"id":"p0v2KKGdgF86"}},{"cell_type":"code","source":["# 📌 Paso 7: Entrenar y evaluar modelos ML\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from sklearn.linear_model import SGDClassifier\n","\n","# Dividimos el dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # stratify=y mantiene el balance de clases y evita problemas de rendimiento.\n","\n","# Entrenamos Naïve Bayes\n","nb_model = MultinomialNB(alpha=0.3)\n","nb_model.fit(X_train, y_train)\n","y_pred_nb = nb_model.predict(X_test)\n","\n","# Entrenamos SVM (Cambiamos el SVC() por SGDClassifier() para acelerar SVM)\n","# svm_model = SVC(kernel='linear', probability=True) -> Es un SVM exacto, resolviendo el problema de optimización completamente pero lento en datasets grandes\n","svm_model = SGDClassifier(loss=\"hinge\", class_weight=\"balanced\", random_state=42) # Más rápido porque usa descenso de gradiente estocástico en lugar de resolver el problema de optimización exacto.\n","svm_model.fit(X_train, y_train)\n","y_pred_svm = svm_model.predict(X_test)\n","\n","# Entrenamos Random Forest\n","rf_model = RandomForestClassifier(\n","    n_estimators=100,  # Aumentamos el número de árboles a 100 para mayor estabilidad\n","    max_depth=12,  # Permitimos árboles más profundos para capturar relaciones complejas\n","    min_samples_split=2,  # Menos restricciones en las divisiones de los árboles\n","    min_samples_leaf=1,  # Permite hojas con menos muestras\n","    class_weight=\"balanced\",  # Maneja automáticamente clases desbalanceadas\n","    random_state=42,\n","    n_jobs=-1  # Usa todos los núcleos del CPU\n",")\n","rf_model.fit(X_train, y_train)\n","y_pred_rf = rf_model.predict(X_test)"],"metadata":{"id":"lyFdibKbsIit","executionInfo":{"status":"ok","timestamp":1741375176122,"user_tz":-60,"elapsed":8211,"user":{"displayName":"Marcos Cifuentes","userId":"03726672031011744657"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["# 📌 Paso 8: Métricas de evaluación de la impresión"],"metadata":{"id":"iDVQEUE-gPda"}},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GauhpjHzeMve","executionInfo":{"status":"ok","timestamp":1741375178770,"user_tz":-60,"elapsed":795,"user":{"displayName":"Marcos Cifuentes","userId":"03726672031011744657"}},"outputId":"d4da9cec-1117-43b1-d942-71fb494730a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["🔹 Naïve Bayes Results:\n","               precision    recall  f1-score   support\n","\n","         bad       0.73      0.96      0.83     21513\n","        good       0.73      0.64      0.68     11197\n","     neutral       0.58      0.31      0.40     11092\n","\n","    accuracy                           0.71     43802\n","   macro avg       0.68      0.63      0.64     43802\n","weighted avg       0.69      0.71      0.68     43802\n","\n","🔹 SVM Results:\n","               precision    recall  f1-score   support\n","\n","         bad       0.80      0.95      0.87     21513\n","        good       0.69      0.84      0.76     11197\n","     neutral       0.72      0.31      0.43     11092\n","\n","    accuracy                           0.76     43802\n","   macro avg       0.74      0.70      0.69     43802\n","weighted avg       0.75      0.76      0.73     43802\n","\n","🔹 Random Forest Results:\n","               precision    recall  f1-score   support\n","\n","         bad       0.75      0.89      0.81     21513\n","        good       0.67      0.74      0.71     11197\n","     neutral       0.60      0.32      0.42     11092\n","\n","    accuracy                           0.71     43802\n","   macro avg       0.67      0.65      0.65     43802\n","weighted avg       0.69      0.71      0.69     43802\n","\n"]}],"source":["# 📌 Paso 8: Métricas de evaluación de la impresión\n","print(\"🔹 Naïve Bayes Results:\\n\", classification_report(y_test, y_pred_nb))\n","print(\"🔹 SVM Results:\\n\", classification_report(y_test, y_pred_svm))\n","print(\"🔹 Random Forest Results:\\n\", classification_report(y_test, y_pred_rf))"]}]}